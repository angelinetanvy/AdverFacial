# -*- coding: utf-8 -*-
"""CASME2 - Learnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hjge9MERBYqaWFLZKSKNsZa-jbmJJZ70

## Learnet ME Models
"""
 
from keras.models import Model
from keras.layers import Input,concatenate,Flatten,Dense,add,BatchNormalization,Dropout
from keras.layers.convolutional import Conv2D


def build(height=224,width=224,channels=76,classes =3):

    im =Input(shape=(224,224,76))
    Conv_S=Conv2D(16, (3,3), activation='relu', padding='same', strides=2, name='Conv_S')(im)
    #-------------------------------------------------------------------
    
    Conv_1_1=Conv2D(16, (1,1), activation='relu', padding='same', strides=2, name='Conv_1_1')(Conv_S)
    Conv_1_2=Conv2D(32, (3,3), activation='relu', padding='same', strides=2, name='Conv_1_2')(Conv_1_1)
    Conv_1_3=Conv2D(64, (5,5), activation='relu', padding='same', strides=2, name='Conv_1_3')(Conv_1_2)
    #------------------------------------------------------------------
    
    Conv_2_1=Conv2D(16, (1,1), activation='relu', padding='same', strides=2, name='Conv_2_1')(Conv_S)
    add_2_1=add([Conv_1_1, Conv_2_1])
    batch_r11=BatchNormalization()(add_2_1)
    Conv_2_2=Conv2D(32, (3,3), activation='relu', padding='same', strides=2, name='Conv_2_2')(batch_r11)
    add_2_2=add([Conv_1_2, Conv_2_2])
    batch_r12=BatchNormalization()(add_2_2)
    Conv_x_2=Conv2D(64, (5,5), activation='relu', padding='same', strides=2, name='Conv_x_2')(batch_r12)
    #------------------------------------------------------------------
    
    Conv_3_1=Conv2D(16, (1,1), activation='relu', padding='same', strides=2, name='Conv_3_1')(Conv_S)
    Conv_3_2=Conv2D(32, (3,3), activation='relu', padding='same', strides=2, name='Conv_3_2')(Conv_3_1)
    Conv_3_3=Conv2D(64, (5,5), activation='relu', padding='same', strides=2, name='Conv_3_3')(Conv_3_2)
    #------------------------------------------------------------------
    
    Conv_4_1=Conv2D(16, (1,1), activation='relu', padding='same', strides=2, name='Conv_4_1')(Conv_S)
    add_4_1=add([Conv_3_1, Conv_4_1])
    batch_r13=BatchNormalization()(add_4_1)
    Conv_4_2=Conv2D(32, (3,3), activation='relu', padding='same', strides=2, name='Conv_4_2')(batch_r13)
    add_4_2=add([Conv_3_2, Conv_4_2])
    batch_r14=BatchNormalization()(add_4_2)
    Conv_x_4=Conv2D(64, (5,5), activation='relu', padding='same', strides=2, name='Conv_x_4')(batch_r14)
   
    #--------------------------------------------------------
    concta1=concatenate([Conv_1_3, Conv_x_2, Conv_3_3, Conv_x_4])
    batch_X=BatchNormalization()(concta1)
    
    #-----------------------------------------------------#    
    Conv_5_1=Conv2D(256, (3,3), activation='relu', padding='same', strides=2, name='Conv_5_1')(batch_X)
    #-----Fully Connected layer--------
    F1=Flatten()(Conv_5_1)
    FC1=Dense(256,activation='relu')(F1)
    drop=Dropout(0.5)(FC1)
    
    #------clasisfication layer-------
    
    out = Dense(classes, activation='softmax')(drop)
    
    model = Model(inputs=[im],outputs= out)
    return model

# import numpy as np

# def deepfool(image, f, grads, num_classes=10, overshoot=0.02, max_iter=50):

#     """
#        :param image: Image of size HxWx3
#        :param f: feedforward function (input: images, output: values of activation BEFORE softmax).
#        :param grads: gradient functions with respect to input (as many gradients as classes).
#        :param num_classes: num_classes (limits the number of classes to test against, by default = 10)
#        :param overshoot: used as a termination criterion to prevent vanishing updates (default = 0.02).
#        :param max_iter: maximum number of iterations for deepfool (default = 10)
#        :return: minimal perturbation that fools the classifier, number of iterations that it required, new estimated_label and perturbed image
#     """

#     f_image = np.array(f(image)).flatten()
#     I = (np.array(f_image)).flatten().argsort()[::-1]

#     I = I[0:num_classes]
#     label = I[0]

#     input_shape = image.shape
#     pert_image = image

#     f_i = np.array(f(pert_image)).flatten()
#     k_i = int(np.argmax(f_i))

#     w = np.zeros(input_shape)
#     r_tot = np.zeros(input_shape)

#     loop_i = 0

#     while k_i == label and loop_i < max_iter:

#         pert = np.inf
#         gradients = np.asarray(grads(pert_image,I))

#         for k in range(1, num_classes):

#             # set new w_k and new f_k
#             w_k = gradients[k, :, :, :, :] - gradients[0, :, :, :, :]
#             f_k = f_i[I[k]] - f_i[I[0]]
#             pert_k = abs(f_k)/np.linalg.norm(w_k.flatten())

#             # determine which w_k to use
#             if pert_k < pert:
#                 pert = pert_k
#                 w = w_k

#         # compute r_i and r_tot
#         r_i =  pert * w / np.linalg.norm(w)
#         r_tot = r_tot + r_i

#         # compute new perturbed image
#         pert_image = image + (1+overshoot)*r_tot
#         loop_i += 1

#         # compute new label
#         f_i = np.array(f(pert_image)).flatten()
#         k_i = int(np.argmax(f_i))

#     r_tot = (1+overshoot)*r_tot

#     return r_tot, loop_i, k_i, pert_image

import os
import cv2
import numpy
import imageio
from sklearn.metrics import confusion_matrix
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution3D, MaxPooling3D
from keras.callbacks import ModelCheckpoint
from keras.utils import np_utils, generic_utils
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from keras import backend as K
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torchvision import transforms
import torchvision
import time
# import Learnet_Model



# training_list = []

# training_list = []
# disgustpath = '/content/drive/MyDrive/Casme II Sorted/CASME2-SORTED/disgust/'
# fearpath = '/content/drive/MyDrive/Casme II Sorted/CASME2-SORTED/fear/'
# happypath = '/content/drive/MyDrive/Casme II Sorted/CASME2-SORTED/happiness/'
# otherpath = '/content/drive/MyDrive/Casme II Sorted/CASME2-SORTED/other/'
# repressionpath = '/content/drive/MyDrive/Casme II Sorted/CASME2-SORTED/repression/'
# sadnesspath = '/content/drive/MyDrive/Casme II Sorted/CASME2-SORTED/sadness/'
# surprisepath = '/content/drive/MyDrive/Casme II Sorted/CASME2-SORTED/surprise/'


# image_rows, image_columns, image_depth = 224, 224, 96

training_list = []

training_list = []
disgustpath = '/content/drive/MyDrive/FYP/CASME2-SORTED/disgust/'
fearpath = '/content/drive/MyDrive/FYP/CASME2-SORTED/fear/'
happypath = '/content/drive/MyDrive/FYP/CASME2-SORTED/happiness/'
otherpath = '/content/drive/MyDrive/FYP/CASME2-SORTED/other/'
repressionpath = '/content/drive/MyDrive/FYP/CASME2-SORTED/repression/'
sadnesspath = '/content/drive/MyDrive/FYP/CASME2-SORTED/sadness/'
surprisepath = '/content/drive/MyDrive/FYP/CASME2-SORTED/surprise/'


image_rows, image_columns, image_depth = 224, 224, 76

directorylisting = os.listdir(disgustpath)
for video in directorylisting:
	frames = []
	videopath = disgustpath + video
	loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
	framerange = [x for x in range(76)]
	for frame in framerange:
		image = loadedvideo.get_data(frame)
		imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
		grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)
		frames.append(grayimage)
	frames = numpy.asarray(frames)
	videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
	training_list.append(videoarray)


print(len(training_list))

directorylisting = os.listdir(fearpath)
for video in directorylisting:
	frames = []
	videopath = fearpath + video
	loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
	framerange = [x for x in range(76)]
	for frame in framerange:
		image = loadedvideo.get_data(frame)
		imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
		grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)
		frames.append(grayimage)
	frames = numpy.asarray(frames)
	videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
	training_list.append(videoarray)
 
print(len(training_list))

directorylisting = os.listdir(happypath)
for video in directorylisting:
	frames = []
	videopath = happypath + video
	loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
	framerange = [x for x in range(76)]
	if len(loadedvideo)>=76:
		for frame in framerange:
									image = loadedvideo.get_data(frame)
									imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
									grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)
									frames.append(grayimage)
		frames = numpy.asarray(frames)
		videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
		training_list.append(videoarray)
 
print(len(training_list))

directorylisting = os.listdir(otherpath)
for video in directorylisting:
	frames = []
	videopath = otherpath + video
	loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
	framerange = [x for x in range(76)]
	if len(loadedvideo) >= 76:
		for frame in framerange:
									image = loadedvideo.get_data(frame)
									imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
									grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)
									frames.append(grayimage)
		frames = numpy.asarray(frames)
		videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
		training_list.append(videoarray)
 
print(len(training_list))

directorylisting = os.listdir(repressionpath)
for video in directorylisting:
	frames = []
	videopath = repressionpath + video
	loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
	framerange = [x for x in range(76)]
	if len(loadedvideo) >= 76:
		for frame in framerange:
									image = loadedvideo.get_data(frame)
									imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
									grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)
									frames.append(grayimage)
		frames = numpy.asarray(frames)
		videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
		training_list.append(videoarray)
 
print(len(training_list))

directorylisting = os.listdir(sadnesspath)
for video in directorylisting:
	frames = []
	videopath = sadnesspath + video
	loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
	framerange = [x for x in range(76)]
	if len(loadedvideo) >= 76:
		for frame in framerange:
									image = loadedvideo.get_data(frame)
									imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
									grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)
									frames.append(grayimage)
		frames = numpy.asarray(frames)
		videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
		training_list.append(videoarray)
 
print(len(training_list))

directorylisting = os.listdir(surprisepath)
for video in directorylisting:
	frames = []
	videopath = surprisepath + video
	loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
	framerange = [x for x in range(76)]
	if len(loadedvideo) >= 76:
		for frame in framerange:
									image = loadedvideo.get_data(frame)
									imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
									grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)
									frames.append(grayimage)
		frames = numpy.asarray(frames)
		videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
		training_list.append(videoarray)
 
print(len(training_list))

print(len(training_list))

print(training_list[0].shape)

training_list = numpy.asarray(training_list)
trainingsamples = len(training_list)

traininglabels = numpy.zeros((trainingsamples, ), dtype = int)

# print(training_list[0].shape)
# print(training_list_new.shape)

traininglabels[0:63] = 0
traininglabels[63:65] = 1
traininglabels[65:96] = 2
traininglabels[96:193] = 3
traininglabels[193:220] = 4
traininglabels[220:224] = 5
traininglabels[224:251] = 6

traininglabels = np_utils.to_categorical(traininglabels, 7)

training_data = [training_list, traininglabels]
(trainingframes, traininglabels) = (training_data[0], training_data[1])
training_set = numpy.zeros((trainingsamples, image_rows, image_columns, image_depth))
for h in range(trainingsamples):
	training_set[h][:][:][:] = trainingframes[h,:,:,:]

print(traininglabels.shape)
print(traininglabels[:10])

# training_set = numpy.zeros((trainingsamples, image_rows, image_columns, image_depth))

print(trainingsamples)
print(trainingframes.shape)
print(training_set.shape)

# for h in range(trainingsamples):
# 	training_set[h][:][:][:] = trainingframes[h,:,:,:]

training_set = training_set.astype('float32')
training_set -= numpy.mean(training_set)
training_set /= numpy.max(training_set)

# # Save training images and labels in a numpy array
# numpy.save('/content/drive/MyDrive/Casme II Sorted/result/microexp_learnnet_images.npy', training_set)
# numpy.save('/content/drive/MyDrive/Casme II Sorted/result/microexp_learnnet_labels.npy', traininglabels)

x_train, x_test, y_train, y_test =  train_test_split(training_set, traininglabels, test_size=0.2, random_state=4)

print(y_test.shape)

x_train.shape

# Learnet model
model = build(height=224,width=224,channels=76,classes =7)
model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])
model.summary()

# Training the model
hist = model.fit(x_train, y_train, batch_size = 25, epochs = 100, shuffle=True)

# Finding Confusion Matrix using pretrained weights
predictions = model.predict(x_test)
y_pred = numpy.argmax(predictions, axis=1)
y_true = numpy.argmax(y_test, axis=1)
cfm = confusion_matrix(y_true, y_pred)

print(cfm)

print(y_pred)

print(y_test)

from sklearn.metrics import accuracy_score
accuracy_score(y_true, y_pred)



"""## Universal Adversarial Framing """

import numpy as np

def deepfool(image, f, grads, num_classes=10, overshoot=0.02, max_iter=50):

    """
       :param image: Image of size HxWx3
       :param f: feedforward function (input: images, output: values of activation BEFORE softmax).
       :param grads: gradient functions with respect to input (as many gradients as classes).
       :param num_classes: num_classes (limits the number of classes to test against, by default = 10)
       :param overshoot: used as a termination criterion to prevent vanishing updates (default = 0.02).
       :param max_iter: maximum number of iterations for deepfool (default = 10)
       :return: minimal perturbation that fools the classifier, number of iterations that it required, new estimated_label and perturbed image
    """

    f_image = np.array(f(image)).flatten()
    I = (np.array(f_image)).flatten().argsort()[::-1]

    I = I[0:num_classes]
    label = I[0]

    input_shape = image.shape
    pert_image = image

    f_i = np.array(f(pert_image)).flatten()
    k_i = int(np.argmax(f_i))

    w = np.zeros(input_shape)
    r_tot = np.zeros(input_shape)

    loop_i = 0

    while k_i == label and loop_i < max_iter:

        pert = np.inf
        gradients = np.asarray(grads(pert_image,I))

        for k in range(1, num_classes):

            # set new w_k and new f_k
            w_k = gradients[k, :, :, :, :] - gradients[0, :, :, :, :]
            f_k = f_i[I[k]] - f_i[I[0]]
            pert_k = abs(f_k)/np.linalg.norm(w_k.flatten())

            # determine which w_k to use
            if pert_k < pert:
                pert = pert_k
                w = w_k

        # compute r_i and r_tot
        r_i =  pert * w / np.linalg.norm(w)
        r_tot = r_tot + r_i

        # compute new perturbed image
        pert_image = image + (1+overshoot)*r_tot
        loop_i += 1

        # compute new label
        f_i = np.array(f(pert_image)).flatten()
        k_i = int(np.argmax(f_i))

    r_tot = (1+overshoot)*r_tot

    return r_tot, loop_i, k_i, pert_image

import numpy as np
def proj_lp(v, xi, p):

    # Project on the lp ball centered at 0 and of radius xi

    # SUPPORTS only p = 2 and p = Inf for now
    if p == 2:
        v = v * min(1, xi/np.linalg.norm(v.flatten(1)))
        # v = v / np.linalg.norm(v.flatten(1)) * xi
    elif p == np.inf:
        v = np.sign(v) * np.minimum(abs(v), xi)
    else:
         raise ValueError('Values of p different from 2 and Inf are currently not supported...')

    return v

def universal_perturbation(dataset, f, grads, delta=0.2, max_iter_uni = np.inf, xi=10, p=np.inf, num_classes=10, overshoot=0.02, max_iter_df=10):
    """
    :param dataset: Images of size MxHxWxC (M: number of images)
    :param f: feedforward function (input: images, output: values of activation BEFORE softmax).
    :param grads: gradient functions with respect to input (as many gradients as classes).
    :param delta: controls the desired fooling rate (default = 80% fooling rate)
    :param max_iter_uni: optional other termination criterion (maximum number of iteration, default = np.inf)
    :param xi: controls the l_p magnitude of the perturbation (default = 10)
    :param p: norm to be used (FOR NOW, ONLY p = 2, and p = np.inf ARE ACCEPTED!) (default = np.inf)
    :param num_classes: num_classes (limits the number of classes to test against, by default = 10)
    :param overshoot: used as a termination criterion to prevent vanishing updates (default = 0.02).
    :param max_iter_df: maximum number of iterations for deepfool (default = 10)
    :return: the universal perturbation.
    """

    v = 0
    fooling_rate = 0.0
    num_images =  np.shape(dataset)[0] # The images should be stacked ALONG FIRST DIMENSION

    itr = 0
    while fooling_rate < 1-delta and itr < max_iter_uni:
        # Shuffle the dataset
        np.random.shuffle(dataset)

        print ('Starting pass number ', itr)

        # Go through the data set and compute the perturbation increments sequentially
        for k in range(0, num_images):
            cur_img = dataset[k:(k+1), :, :, :]

            if int(np.argmax(np.array(f(cur_img)).flatten())) == int(np.argmax(np.array(f(cur_img+v)).flatten())):
                print('>> k = ', k, ', pass #', itr)

                # Compute adversarial perturbation
                dr,iter,_,_ = deepfool(cur_img + v, f, grads, num_classes=num_classes, overshoot=overshoot, max_iter=max_iter_df)

                # Make sure it converged...
                if iter < max_iter_df-1:
                    v = v + dr

                    # Project on l_p ball
                    v = proj_lp(v, xi, p)

        itr = itr + 1

        # Perturb the dataset with computed perturbation
        dataset_perturbed = dataset + v

        est_labels_orig = np.zeros((num_images))
        est_labels_pert = np.zeros((num_images))

        batch_size = 100
        num_batches = np.int(np.ceil(np.float(num_images) / np.float(batch_size)))

        # Compute the estimated labels in batches
        for ii in range(0, num_batches):
            m = (ii * batch_size)
            M = min((ii+1)*batch_size, num_images)
            est_labels_orig[m:M] = np.argmax(f(dataset[m:M, :, :, :]), axis=1).flatten()
            est_labels_pert[m:M] = np.argmax(f(dataset_perturbed[m:M, :, :, :]), axis=1).flatten()

        # Compute the fooling rate
        fooling_rate = float(np.sum(est_labels_pert != est_labels_orig) / float(num_images))
        print('FOOLING RATE = ', fooling_rate)

    return v

original = []
angrypath = '/content/drive/MyDrive/Adverfacial/CAS(ME)2/sortedvideo/angry/'
happypath = '/content/drive/MyDrive/Adverfacial/CAS(ME)2/sortedvideo/happy/'
disgustpath = '/content/drive/MyDrive/Adverfacial/CAS(ME)2/sortedvideo/disgust/'

image_rows, image_columns, image_depth = 224, 224, 96



# !pip install pymatreader



import scipy
import sklearn
from sklearn.feature_extraction import image
from scipy.io import loadmat
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
images = loadmat('/content/drive/MyDrive/Adverfacial/CAS(ME)2/precomputed/VGG-16.mat')

# print(images)

imgplot = plt.imshow(images['r'])

plt.show()

v = images['r']

print(v.shape)
print(v)

# Clip the perturbation to make sure images fit in uint8
# clipped_v = np.clip(undo_image_avg(image_original[0,:,:,:]+v[0,:,:,:]), 0, 255) - np.clip(undo_image_avg(image_original[0,:,:,:]), 0, 255)

# transformer = transforms.ToTensor()
# transformer1 = transforms.Compose([transforms.ToTensor(),])
# transformer2 = transforms.Compose([
#   transforms.Resize(112),
#   # transforms.CenterCrop(28),
# ])

# print(transformer(v).shape)
# print(transformer(v).shape)

def undo_image_avg(img):
    img_copy = np.copy(img)
    img_copy[:, :, 0] = img_copy[:, :, 0] + 123.68
    img_copy[:, :, 1] = img_copy[:, :, 1] + 116.779
    img_copy[:, :, 2] = img_copy[:, :, 2] + 103.939
    return img_copy

def preprocess_image(img, img_size=None, crop_size=None, color_mode="rgb", out=None):
    img = img.astype('float32')

    # We normalize the colors (in RGB space) with the empirical means on the training set
    img[:, :, 0] -= 123.68
    img[:, :, 1] -= 116.779
    img[:, :, 2] -= 103.939

    return img

#  preprocess dataset (64,64,96)
image_rows, image_columns, image_depth = 224,224,96

from torchvision import transforms

# Importing Image module from PIL package 
from PIL import Image 
import PIL 

count =0

directorylisting = os.listdir(angrypath)
for video in directorylisting:
    frames = []
    videopath = angrypath + video
    loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
    framerange = [x + 72 for x in range(96)]
    count+=1
    for frame in framerange:
                  image = loadedvideo.get_data(frame)
                  imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)

                  plt.figure()
                  
                  print("Original Image")
                  plt.subplot(1, 3, 1)
                  plt.imshow((imageresize[:, :, :]).astype(dtype='uint8'), interpolation=None)
                  plt.show()

                  image_original = preprocess_image(imageresize, color_mode="rgb")
                  print("Preprocess Image")
                  plt.subplot(1, 3, 2)
                  plt.imshow((image_original[:, :, :]).astype(dtype='uint8'), interpolation=None)
                  plt.show()

                  # Clip the perturbation to make sure images fit in uint8
                  clipped_v = np.clip(undo_image_avg(image_original[:,:,:]+v[:,:,:]), 0, 255) - np.clip(undo_image_avg(image_original[:,:,:]), 0, 255)
                  # print(clipped_v.shape)

                  image_perturbed = image_original + clipped_v
                  # imageresize  += transformer(v).float()
                  # print(image_perturbed.shape)
                  # print(img_perturb.shape)
                  # frames.append(grayimage)
                  print("Perturbed Image")
                  plt.subplot(1, 3, 3)
                  plt.imshow(undo_image_avg(image_perturbed[:, :, :]).astype(dtype='uint8'), interpolation=None)
                  plt.savefig("/content/drive/MyDrive/Adverfacial/perturbed/angry/" +str(count)+"frame_"+str(frame))
                  plt.show()

                  # plt.savefig("/content/drive/MyDrive/Adverfacial/perturbed/angry/" +str(count)+"frame_"+str(frame))

                  grayimage = cv2.cvtColor(image_perturbed, cv2.COLOR_BGR2GRAY)
                  frames.append(grayimage)
    
    frames = numpy.asarray(frames)
    videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
    original.append(videoarray)


print(len(original))

#  preprocess dataset (64,64,96)
image_rows, image_columns, image_depth = 224,224,96

from torchvision import transforms

count = 0

directorylisting = os.listdir(happypath)
for video in directorylisting:
    frames = []
    videopath = happypath + video
    loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
    framerange = [x + 72 for x in range(96)]
    count+=1
    for frame in framerange:
                  image = loadedvideo.get_data(frame)
                  imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
                  print("Original Image")
                  # plt.subplot(1, 3, 1)
                  # plt.imshow((imageresize[:, :, :]).astype(dtype='uint8'), interpolation=None)
                  # plt.show()

                  image_original = preprocess_image(imageresize, color_mode="rgb")
                  print("Preprocess Image")
                  # plt.subplot(1, 3, 2)
                  # plt.imshow((image_original[:, :, :]).astype(dtype='uint8'), interpolation=None)
                  # plt.show()

                  # Clip the perturbation to make sure images fit in uint8
                  clipped_v = np.clip(undo_image_avg(image_original[:,:,:]+v[:,:,:]), 0, 255) - np.clip(undo_image_avg(image_original[:,:,:]), 0, 255)
                  # print(clipped_v.shape)

                  image_perturbed = image_original + clipped_v
                  
                  # imageresize  += transformer(v).float()
                  # print(image_perturbed.shape)
                  # print(img_perturb.shape)
                  # frames.append(grayimage)
                  print("Perturbed Image")
                  # plt.subplot(1, 3, 3)
                  plt.figure()
                  plt.imshow(undo_image_avg(image_perturbed[:, :, :]).astype(dtype='uint8'), interpolation=None)
                  plt.savefig("/content/drive/MyDrive/Adverfacial/perturbed/happy/" +str(count)+"frame_"+str(frame))
                  # plt.show()

                  grayimage = cv2.cvtColor(image_perturbed, cv2.COLOR_BGR2GRAY)
                  frames.append(grayimage)
    frames = numpy.asarray(frames)
    videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
    original.append(videoarray)


print(len(original))

#  preprocess dataset (64,64,96)
image_rows, image_columns, image_depth = 224,224,96

from torchvision import transforms
count = 0
directorylisting = os.listdir(disgustpath)
for video in directorylisting:
    frames = []
    videopath = disgustpath + video
    loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
    framerange = [x + 72 for x in range(96)]
    count +=1
    for frame in framerange:
                  image = loadedvideo.get_data(frame)
                  imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
                  print("Original Image")
                  # plt.subplot(1, 3, 1)
                  # plt.imshow((imageresize[:, :, :]).astype(dtype='uint8'), interpolation=None)
                  # plt.show()

                  image_original = preprocess_image(imageresize, color_mode="rgb")
                  print("Preprocess Image")
                  # plt.subplot(1, 3, 2)
                  # plt.imshow((image_original[:, :, :]).astype(dtype='uint8'), interpolation=None)
                  # plt.show()

                  # Clip the perturbation to make sure images fit in uint8
                  clipped_v = np.clip(undo_image_avg(image_original[:,:,:]+v[:,:,:]), 0, 255) - np.clip(undo_image_avg(image_original[:,:,:]), 0, 255)
                  # print(clipped_v.shape)

                  image_perturbed = image_original + clipped_v
                  # imageresize  += transformer(v).float()
                  # print(image_perturbed.shape)
                  # print(img_perturb.shape)
                  # frames.append(grayimage)
                  print("Perturbed Image")
                  # plt.subplot(1, 3, 3)
                  plt.figure()
                  plt.imshow(undo_image_avg(image_perturbed[:, :, :]).astype(dtype='uint8'), interpolation=None)
                  plt.savefig("/content/drive/MyDrive/Adverfacial/perturbed/disgust/" +str(count)+"frame_"+str(frame))
                  # plt.show()

                  grayimage = cv2.cvtColor(image_perturbed, cv2.COLOR_BGR2GRAY)
                  frames.append(grayimage)
    frames = numpy.asarray(frames)
    videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
    original.append(videoarray)


print(len(original))

original = numpy.asarray(original)
trainingsamples = len(original)

traininglabels = numpy.zeros((trainingsamples, ), dtype = int)

traininglabels[0:76] = 0
traininglabels[76:170] = 1
traininglabels[170:206] = 2

traininglabels = np_utils.to_categorical(traininglabels, 3)

training_data = [training_list, traininglabels]
(trainingframes, traininglabels) = (training_data[0], training_data[1])
training_set = numpy.zeros((trainingsamples, image_rows, image_columns, image_depth))
for h in range(trainingsamples):
	training_set[h][:][:][:] = trainingframes[h,:,:,:]

training_set = training_set.astype('float32')
training_set -= numpy.mean(training_set)
training_set /= numpy.max(training_set)

# Save training images and labels in a numpy array
numpy.save('/content/drive/MyDrive/Adverfacial/result/microexp_learnnet_images_perturbed.npy', training_set)
numpy.save('/content/drive/MyDrive/Adverfacial/result/microexp_learnnet_labels_perturbed.npy', traininglabels)

x_train, x_test, y_train, y_test =  train_test_split(training_set, traininglabels, test_size=0.2, random_state=4)

print(y_test.shape)

# Finding Confusion Matrix using pretrained weights
predictions = model.predict(x_test)
y_pred = numpy.argmax(predictions, axis=1)
y_true = numpy.argmax(y_test, axis=1)
cfm = confusion_matrix(y_true, y_pred)

print(cfm)

"""# LEARNet CASME2"""

import os
import cv2
import numpy
import imageio
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
from tensorflow.keras.layers import Convolution3D, MaxPooling3D
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from tensorflow.keras import backend as K
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torchvision import transforms
import torchvision
import time

start = time.time()

K.set_image_data_format('channels_first')

image_rows, image_columns, image_depth = 64, 64, 96

training_list = []
disgustpath = '/content/drive/MyDrive/Casme II Sorted/CASME2-SORTED/disgust/'
fearpath = '/content/drive/MyDrive/Casme II Sorted/CASME2-SORTED/fear/'
happypath = '/content/drive/MyDrive/Casme II Sorted/CASME2-SORTED/happiness/'
otherpath = '/content/drive/MyDrive/Casme II Sorted/CASME2-SORTED/other/'
repressionpath = '/content/drive/MyDrive/Casme II Sorted/CASME2-SORTED/repression/'
sadnesspath = '/content/drive/MyDrive/Casme II Sorted/CASME2-SORTED/sadness/'
surprisepath = '/content/drive/MyDrive/Casme II Sorted/CASME2-SORTED/surprise/'
ori_path = '/content/drive/MyDrive/Casme II Sorted/original_learnet/'
v_path = '/content/drive/MyDrive/Casme II Sorted/v_learnet/'
pert_path = '/content/drive/MyDrive/Casme II Sorted/perturbed _learnet/'

directorylisting = os.listdir(disgustpath)
for video in directorylisting:
    frames = []
    videopath = disgustpath + video
    loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
    framerange = [x + 72 for x in range(96)]
    for frame in framerange:
                image = loadedvideo.get_data(frame)
                imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
                grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)

                # framing = imagenet.ImageNet.get_framing(1)
                # input_att, _ = framing(input=img_tensor)
                # with_frame = input_att.numpy()
                # with_frame = cv2.resize(with_frame, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
                # plt.imshow(with_frame)
                # plt.show()
                
                frames.append(grayimage)
    frames = numpy.asarray(frames)
    videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
    training_list.append(videoarray)

directorylisting = os.listdir(fearpath)
for video in directorylisting:
    frames = []
    videopath = fearpath + video
    loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
    framerange = [x + 72 for x in range(96)]
    for frame in framerange:
        image = loadedvideo.get_data(frame)
        imageresize = cv2.resize(image, (image_rows, image_columns), interpolation=cv2.INTER_AREA)
        grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)

        # framing = imagenet.ImageNet.get_framing(1)
        # input_att, _ = framing(input=img_tensor)
        # with_frame = input_att.numpy()
        # with_frame = cv2.resize(with_frame, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
        # plt.imshow(with_frame)
        # plt.show()

        frames.append(grayimage)
    frames = numpy.asarray(frames)
    videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
    training_list.append(videoarray)

directorylisting = os.listdir(happypath)
for video in directorylisting:
        frames = []
        videopath = happypath + video
        loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
        framerange = [x + 72 for x in range(96)]
        for frame in framerange:
                image = loadedvideo.get_data(frame)
                imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
                grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)

                # img_tensor = torch.tensor(grayimage)
                # framing = imagenet.ImageNet.get_framing(1)
                # input_att, _ = framing(input=img_tensor)
                # with_frame = input_att.numpy()
                # with_frame = cv2.resize(with_frame, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
                
                frames.append(grayimage)
        frames = numpy.asarray(frames)
        videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
        training_list.append(videoarray)

directorylisting = os.listdir(otherpath)
for video in directorylisting:
        frames = []
        videopath = otherpath + video
        loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
        framerange = [x + 72 for x in range(96)]
        for frame in framerange:
                image = loadedvideo.get_data(frame)
                imageresize = cv2.resize(image, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
                grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)


                # img_tensor = torch.tensor(grayimage)
                # framing = imagenet.ImageNet.get_framing(1)
                # input_att, _ = framing(input=img_tensor)
                # with_frame = input_att.numpy()
                # with_frame = cv2.resize(with_frame, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
                
                frames.append(grayimage)
        frames = numpy.asarray(frames)
        videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
        training_list.append(videoarray)

directorylisting = os.listdir(repressionpath)
for video in directorylisting:
    frames = []
    videopath = repressionpath + video
    loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
    framerange = [x + 72 for x in range(96)]
    for frame in framerange:
        image = loadedvideo.get_data(frame)
        imageresize = cv2.resize(image, (image_rows, image_columns), interpolation=cv2.INTER_AREA)
        grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)

        # framing = imagenet.ImageNet.get_framing(1)
        # input_att, _ = framing(input=img_tensor)
        # with_frame = input_att.numpy()
        # with_frame = cv2.resize(with_frame, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
        # plt.imshow(with_frame)
        # plt.show()

        frames.append(grayimage)
    frames = numpy.asarray(frames)
    videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
    training_list.append(videoarray)

directorylisting = os.listdir(sadnesspath)
for video in directorylisting:
    frames = []
    videopath = sadnesspath + video
    loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
    framerange = [x + 72 for x in range(96)]
    for frame in framerange:
        image = loadedvideo.get_data(frame)
        imageresize = cv2.resize(image, (image_rows, image_columns), interpolation=cv2.INTER_AREA)
        grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)

        # framing = imagenet.ImageNet.get_framing(1)
        # input_att, _ = framing(input=img_tensor)
        # with_frame = input_att.numpy()
        # with_frame = cv2.resize(with_frame, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
        # plt.imshow(with_frame)
        # plt.show()

        frames.append(grayimage)
    frames = numpy.asarray(frames)
    videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
    training_list.append(videoarray)

directorylisting = os.listdir(surprisepath)
for video in directorylisting:
    frames = []
    videopath = surprisepath + video
    loadedvideo = imageio.get_reader(videopath, 'ffmpeg')
    framerange = [x + 72 for x in range(96)]
    for frame in framerange:
        image = loadedvideo.get_data(frame)
        imageresize = cv2.resize(image, (image_rows, image_columns), interpolation=cv2.INTER_AREA)
        grayimage = cv2.cvtColor(imageresize, cv2.COLOR_BGR2GRAY)

        # framing = imagenet.ImageNet.get_framing(1)
        # input_att, _ = framing(input=img_tensor)
        # with_frame = input_att.numpy()
        # with_frame = cv2.resize(with_frame, (image_rows, image_columns), interpolation = cv2.INTER_AREA)
        # plt.imshow(with_frame)
        # plt.show()

        frames.append(grayimage)
    frames = numpy.asarray(frames)
    videoarray = numpy.rollaxis(numpy.rollaxis(frames, 2, 0), 2, 0)
    training_list.append(videoarray)

training_list = numpy.asarray(training_list)
trainingsamples = len(training_list)

traininglabels = numpy.zeros((trainingsamples, ), dtype = int)

traininglabels[0:76] = 0
traininglabels[76:170] = 1
traininglabels[170:206] = 2

traininglabels = np_utils.to_categorical(traininglabels, 3)

training_data = [training_list, traininglabels]
(trainingframes, traininglabels) = (training_data[0], training_data[1])
training_set = numpy.zeros((trainingsamples, 1, image_rows, image_columns, image_depth))
for h in range(trainingsamples):
    training_set[h][0][:][:][:] = trainingframes[h,:,:,:]

training_set = training_set.astype('float32')
training_set -= numpy.mean(training_set)
training_set /= numpy.max(training_set)

# Save training images and labels in a numpy array
# numpy.save('numpy_training_datasets/microexpstcnn_images.npy', training_set)
# numpy.save('numpy_training_datasets/microexpstcnn_labels.npy', traininglabels)

# Load training images and labels that are stored in numpy array
"""
training_set = numpy.load('numpy_training_datasets/microexpstcnn_images.npy')
traininglabels =numpy.load('numpy_training_datasets/microexpstcnn_labels.npy')
"""


# Learnet model
model = build(height=64,width=64,channels=96,classes =3)
model.compile(loss = 'categorical_crossentropy', optimizer = 'SGD', metrics = ['accuracy'])
model.summary()

# Load pre-trained weights
# """
#model.load_weights('weights_microexpstcnn/weights-improvement-53-0.88.hdf5')
# """

filepath="weights_microexpstcnn/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]

training_set2 = training_set.copy()
training_set.resize(96,64,64,96)

# Spliting the dataset into training and validation sets
train_images, validation_images, train_labels, validation_labels =  train_test_split(training_set, traininglabels, test_size=0.5, random_state=2)

# Save validation set in a numpy array
"""
numpy.save('numpy_validation_dataset/microexpstcnn_val_images.npy', validation_images)
numpy.save('numpy_validation_dataset/microexpstcnn_val_labels.npy', validation_labels)
"""

# Load validation set from numpy array
"""
validation_images = numpy.load('numpy_validation_datasets/microexpstcnn_val_images.npy')
validation_labels = numpy.load('numpy_validation_datasets/microexpstcnn_val_labels.npy')
"""

# Training the model
hist = model.fit(train_images, train_labels, validation_data = (validation_images, validation_labels), callbacks=callbacks_list, batch_size = 16, nb_epoch = 100, shuffle=True)

# Finding Confusion Matrix using pretrained weights
predictions = model.predict(validation_images)
predictions_labels = numpy.argmax(predictions, axis=1)
validation_labels = numpy.argmax(validation_labels, axis=1)
cfm = confusion_matrix(validation_labels, predictions_labels)
if len(cfm)==3: fooling_rate = (cfm[0][0]+cfm[1][1]+cfm[2][2])/48
else: fooling_rate = (cfm[0][0]+cfm[1][1])/48
print (cfm)
print(fooling_rate)

# def project_perturbation(data_point,p,perturbation ):
#     if p == 2:
#         perturbation = perturbation * min(1, data_point / np.linalg.norm(perturbation.flatten(1)))
#     elif p == np.inf:
#         perturbation = np.sign(perturbation) * np.minimum(abs(perturbation), data_point)
#     return perturbation

# delta = 0.2
# max_iter_uni=20
# num_classes = 10
# overshoot=0.2
# max_iter_df=20
# xi=10
# p=np.inf
# fooling_rate = 0.0
# iter = 0
# v=np.zeros([64,64])
# net = mdl.ConvNet()
# #validation_labels = numpy.argmax(validation_labels, axis=1)
# transformer = transforms.ToTensor()
# train_images2, validation_images2, train_labels2, validation_labels2 =  train_test_split(training_set2, traininglabels, test_size=0.5, random_state=2)
# validation_labels2 = numpy.argmax(validation_labels2, axis=1)

# while fooling_rate < 1-delta and iter < max_iter_uni:
#         print("Iteration  ", iter)
#         predictions = model.predict(validation_images)
#         predictions_labels = numpy.argmax(predictions, axis=1)


#         path1 = os.path.join(ori_path,str(iter)+"/")    
#         os.mkdir(path1)

#         path3 = os.path.join(pert_path,str(iter)+"/")    
#         os.mkdir(path3)
#         for index in range (len(validation_labels)):
#                 v = v.reshape((v.shape[0], -1))

#                 # Feeding the original image to the network and storing the label returned
#                 r2 = validation_labels[index]

#                 # # Generating a perturbed image from the current perturbation v and the original image
#                 # per_img = Image.fromarray(transformer2(cur_img)+v.astype(np.uint8))
#                 # per_img1 = transformer1(transformer2(per_img))[np.newaxis, :].to(device)

#                 # Feeding the perturbed image to the network and storing the label returned
#                 r1 = predictions_labels[index]

#                 # If the label of both images is the same, the perturbation v needs to be updated
#                 if r1 == r2:
#                         print(">> k =", index, ', pass #', iter, end='      ')

#                 # Finding a new minimal perturbation with deepfool to fool the network on this image
#                 img_tensor = torch.tensor(validation_images2[index])

#                 path2 = os.path.join(path1+str(index)+"/")
#                 os.mkdir(path2)
#                 for i in range(96):
#                         name = str(iter)+"/"+str(index)+"/"+str(i)+".jpg"
#                         frame_img = img_tensor[:, :, :, i]
                        
#                         torchvision.utils.save_image(frame_img, ori_path+name, normalize = True)

#                         dr, iter_k, label, k_i, pert_image = deepfool.deepfool(frame_img, net, num_classes=num_classes, overshoot=overshoot, max_iter=max_iter_df)

#                         # Adding the new perturbation found and projecting the perturbation v and data point xi on p.
#                         if iter_k < max_iter_df-1:

#                                 # framing = imagenet.ImageNet.get_framing(1)
#                                 # input_att, _ = framing(input=img_tensor)
#                                 # with_frame = input_att.numpy()
#                                 # with_frame = cv2.resize(with_frame, (image_rows, image_columns), interpolation = cv2.INTER_AREA)

#                                 v[:, :] += dr[0,0, :, :]

#                                 v = project_perturbation( xi, p,v)

#         name = str(iter)+".jpg"
#         torchvision.utils.save_image(transformer(v), v_path+name, normalize = True)
#         val_imgs = validation_images2.copy()
#         for i in range (len(validation_images2)):
#                 path4 = os.path.join(path3,str(i)+"/")    
#                 os.mkdir(path4)
#                 val_img = validation_images2[i]
#                 val_img_tensor = torch.tensor(validation_images2[i])
#                 for j in range(96):
#                         name = str(iter)+"/"+str(i)+"/"+str(j)+".jpg"
#                         frame_img_tensor = val_img_tensor[:, :, :, i]
#                         frame_img_tensor += transformer(v).float()

#                         frame_img = val_img[:, :, :, i]
#                         frame_img += v

#                         torchvision.utils.save_image(frame_img_tensor, pert_path+name, normalize = True)
#                         val_imgs[i][:, :, :, i] = frame_img       
#         val_imgs.resize(48,64,64,96)        
#         predictions = model.predict(val_imgs)
#         predictions_labels = numpy.argmax(predictions, axis=1)
#         cfm = confusion_matrix(validation_labels2, predictions_labels)
#         print(cfm)
#         if len(cfm)==3: fooling_rate = (cfm[0][0]+cfm[1][1]+cfm[2][2])/48
#         else: fooling_rate = (cfm[0][0]+cfm[1][1])/48
#         print(fooling_rate)
#         iter = iter + 1

# end = time.time()

# print(" ============ Time taken :",end=" ")
# print(end-start,end =" ===========\n")